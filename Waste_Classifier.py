# -*- coding: utf-8 -*-
"""Garbage_org.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1iOBa7Boh3MCn88-Alevk99OU5shssHiD

VIREN KATHPAL

I've chosen to use transfer learning with the MobileNetV2 architecture from TensorFlow's Keras applications.

MobileNetV2 is a convolutional neural network (CNN) architecture designed for efficient and lightweight deep learning tasks. It is particularly well-suited for mobile and edge devices due to its small size and computational efficiency.

Why MobileNetV2 architecture from TensorFlow's Keras applications?


MobileNetV2 is a lightweight and efficient neural network architecture, making it suitable for tasks like object detection on resource-constrained devices. Transfer learning with pre-trained models can significantly speed up development and improve performance, especially when working with limited labeled data.

Setting Up Colab Environment
"""

# Install cvzone
!pip install cvzone

"""These are commonly used libraries for computer vision tasks, including image processing and deep learning."""

# Install TensorFlow
!pip install tensorflow

# Install OpenCV
!pip install opencv-python

import cvzone
import tensorflow as tf
import cv2
import numpy as np
import matplotlib.pyplot as plt

"""Mounting google drive"""

from google.colab import drive
drive.mount('/content/drive')

"""Paths for training and testing datasets."""

train_path='/content/drive/MyDrive/train'
test_path='/content/drive/MyDrive/test'

"""Installation of the keras library and the import of various modules and libraries needed for the project.

Why Keras?

The use of the Keras library in my project provides several advantages for building and training deep learning models. Here are some reasons why Keras is beneficial for your waste management project:

High-Level API: Keras is a high-level neural networks API that is user-friendly and allows for easy and fast prototyping.

Modularity: Keras allows to build models by assembling layers, making it easy to experiment with different architectures.

Compatibility with TensorFlow: Keras is integrated into TensorFlow, one of the most popular deep learning frameworks. This integration allows me to seamlessly combine the simplicity of Keras with the flexibility and scalability of TensorFlow.


Pre-trained Models: Keras provides access to a variety of pre-trained models through its applications module. In my project, I've mentioned using transfer learning with the MobileNetV2 architecture. Keras simplifies the process of loading pre-trained models, making it convenient for tasks like object detection and classification.


Abstraction of Training Details: Keras abstracts away many of the training details, such as optimization algorithms, loss functions, and metrics.
"""

pip install -q keras

import keras

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import os
import cv2
import sklearn
from PIL import Image as im
from glob import glob
from sklearn.model_selection import train_test_split
import keras
from keras.utils import to_categorical
from keras.models import Sequential, load_model
from keras.models import Model
from keras.applications.vgg16 import VGG16
from keras.models import Model
from keras.applications.resnet50 import ResNet50
from keras.applications.vgg16 import preprocess_input
from keras.layers import Conv2D, MaxPool2D, Dense, Flatten, Dropout
from keras.preprocessing.image import ImageDataGenerator
from keras.optimizers import Adam
from keras.callbacks import ModelCheckpoint, EarlyStopping

"""Load and Preprocess the Data

I have used 1185 images for train dataset  and 301 images for test dataset and i have made three classes ->plastic,paper ,metal.

Train Split Ratio= Number of Training Samples/Total Number of Samples

Train Split Ratio=1185/1185+301
Train Split Ratio=0.796


Test Split Ratio= Number of Test Samples/Total Number of Samples
​

Test Split Ratio= 301/1185+301
Train Split Ratio=0.204

So, my train to test split ratio is approximately 79.6% for training and 20.4% for testing.

jpg pics have been used by me.

To give you a rough idea for the test dataset
Paper->117 pics
Metal->80 pics
Plastic->104 pics
Total->301 pics

Similarly We can assume it for training dataset.
Total Pics->1185
"""

# Load and Preprocess the Data:
# Import necessary modules
from keras.applications.mobilenet_v2 import MobileNetV2, preprocess_input

# Define image size for MobileNetV2
img_size = (224, 224)

# Create data generators for training and testing, including data augmentation for training
train_datagen = ImageDataGenerator(
    preprocessing_function=preprocess_input,   # Apply MobileNetV2-specific preprocessing
    shear_range=0.2,  # Apply shear transformation
    zoom_range=0.2,   # Apply zoom transformation
    horizontal_flip=True  # Apply horizontal flip
)
# Create a data generator for testing without data augmentation
test_datagen = ImageDataGenerator(
    preprocessing_function=preprocess_input
)

# Generate batches of augmented data for training
train_generator = train_datagen.flow_from_directory(
    train_path,  # Path to the training dataset directory
    target_size=img_size,   # Resize images to (224, 224) as expected by MobileNetV2
    batch_size=32,     # Number of images in each batch
    class_mode='categorical'  # Use categorical mode for multiple classes
)

# Generate batches of data for testing (without data augmentation)
test_generator = test_datagen.flow_from_directory(
    test_path,  # Path to the testing dataset directory
    target_size=img_size,  # Resize images to (224, 224) as expected by MobileNetV2
    batch_size=32,   # Number of images in each batch
    class_mode='categorical'   # Use categorical mode for multiple classes
)

"""Why Data augmentation?

Data augmentation serves several important purposes in the context of training machine learning models, especially for computer vision tasks like image classification. Here are some key reasons why data augmentation is widely used:
​

Increased Dataset Size

Improved Model Generalization

Reduced Overfitting

Increased Robustness

Learning Invariance

Cost-Efficient Training

Better Training Convergence

Data augmentation is a valuable technique that contributes to the development of more robust and generalizable machine learning models.

Why data augmentation is used for training purpose and not for testing purpose?

Data augmentation is a training-specific technique designed to enhance the model's ability to generalize to diverse conditions.
During testing, the model is evaluated on its performance with respect to the original, unaltered data, providing a reliable measure of its true generalization capabilities.



​

# Load MobileNetV2 Model

Load MobileNetV2 Model:

This code loads the MobileNetV2 model pre-trained on the ImageNet dataset. It excludes the top (classification) layers (include_top=False) and specifies the input shape as (224, 224, 3) to match the expected input size for MobileNetV2.
"""

# Load MobileNetV2 Model
base_model = MobileNetV2(weights='imagenet', include_top=False, input_shape=(224, 224, 3))

# Count the number of subdirectories in the training path (assuming one subdirectory per class)
num_classes = len(os.listdir(train_path))

"""This code builds a custom model for waste classification using the MobileNetV2 base. It adds a Flatten layer to convert the 4D output from MobileNetV2 into 2D, followed by a Dense layer with 256 neurons and ReLU activation. Dropout is applied to prevent overfitting, and the final Dense layer outputs probabilities for each waste category."""

# Build Custom Model for Waste Classification:
model = Sequential([
    base_model,
    Flatten(),
    Dense(256, activation='relu'),
    Dropout(0.5),
    Dense(num_classes, activation='softmax')   # num_classes is the number of waste categories
])

# Freeze MobileNetV2 layers
# This code freezes the weights of the MobileNetV2 layers to retain the pre-trained knowledge and
# prevent them from being updated during training. This is often done when using transfer learning to keep the features learned by the pre-trained model.
for layer in base_model.layers:
    layer.trainable = False




# Compile the Model:
# This code compiles the model using the Adam optimizer with a learning rate of 0.001, categorical crossentropy as the loss function (suitable for multi-class classification),
# and accuracy as the evaluation metric.
model.compile(optimizer=Adam(lr=0.001), loss='categorical_crossentropy', metrics=['accuracy'])

"""Train the Model:

Explanation

train_generator and test_generator: These are instances of ImageDataGenerator that generate batches of augmented images for training and testing.

steps_per_epoch: It determines the number of batches per epoch during training. The formula samples  batch_size calculates the number of batches required to process the entire training dataset once.

epochs: It specifies the number of times the model will iterate over the entire training dataset.

validation_data: It is the generator for the validation dataset, and the model will be evaluated on this dataset after each training epoch.

validation_steps:It determines the number of batches used for validation. Samples batch_size calculates the number of batches for processing the entire validation dataset once.
"""

history = model.fit(
    train_generator,   # Training data generator
    steps_per_epoch=train_generator.samples // train_generator.batch_size,  # Number of steps per epoch
    epochs=10,  # Adjust the number of epochs based on your dataset and training requirements   # Number of training epochs
    validation_data=test_generator,  # Validation data generator
    validation_steps=test_generator.samples // test_generator.batch_size  # Number of steps for validation
)

"""This line uses the save method of the Keras model to save the entire model (including architecture, weights, and optimizer state) to an HDF5 file."""

model.save('/content/drive/MyDrive/waste_classification_model.h5')

"""Load the Trained Model

This code utilizes the load_model function from Keras to load a pre-trained model saved in the HDF5 format. The waste_model variable now holds the loaded model, and now i can use it for making predictions on new data for further evaluation.
"""

from keras.models import load_model

# Load the trained model
model_path = '/content/drive/MyDrive/waste_classification_model.h5'
waste_model = load_model(model_path)

"""Object Detection with OpenCV

Why OpenCv with object detection is necessary for this project?

Although OpenCV with object detection is not strictly necessary for a waste classification project, but it can be a valuable tool for this project.Here are some reasons that why i have consider using OpenCV with object detection:

Object Localization: Precisely locate and identify waste objects within an image.

Visualization: OpenCV can be helpful for visualizing the results of the model.
"""

import os

# Directory containing multiple waste disposal area images
images_directory='/content/drive/MyDrive/train/plastic'

# Iterate through all files in the directory
for filename in os.listdir(images_directory):
    if filename.endswith('.jpg') or filename.endswith('.png'):
        # Construct the full path to the image
        image_path = os.path.join(images_directory, filename)

        # Load the input image
        image = cv2.imread(image_path)

        # Check if the image is loaded successfully
        if image is None:
            print(f"Error: Unable to load the image at path {image_path}")
        else:
            # Convert the image to RGB (OpenCV uses BGR by default)
            image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)

            # Rest of the code for object detection and classification...
            # (Use the same code from the previous example)

            # Display the result
            plt.imshow(image)
            plt.title(f"Waste Image: {filename}")
            plt.show()

import os

# Directory containing multiple waste disposal area images
images_directory = '/content/drive/MyDrive/train/metal'



# Iterate through all files in the directory
for filename in os.listdir(images_directory):
    if filename.endswith('.jpg') or filename.endswith('.png'):
        # Construct the full path to the image
        image_path = os.path.join(images_directory, filename)

        # Load the input image
        image = cv2.imread(image_path)

        # Check if the image is loaded successfully
        if image is None:
            print(f"Error: Unable to load the image at path {image_path}")
        else:
            # Convert the image to RGB (OpenCV uses BGR by default)
            image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)

            # Rest of the code for object detection and classification...
            # (Use the same code from the previous example)

            # Display the result
            plt.imshow(image)
            plt.title(f"Waste Image: {filename}")
            plt.show()

import os

# Directory containing multiple waste disposal area images
images_directory='/content/drive/MyDrive/train/paper'



# Iterate through all files in the directory
for filename in os.listdir(images_directory):
    if filename.endswith('.jpg') or filename.endswith('.png'):
        # Construct the full path to the image
        image_path = os.path.join(images_directory, filename)

        # Load the input image
        image = cv2.imread(image_path)

        # Check if the image is loaded successfully
        if image is None:
            print(f"Error: Unable to load the image at path {image_path}")
        else:
            # Convert the image to RGB (OpenCV uses BGR by default)
            image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)

            # Rest of the code for object detection and classification...
            # (Use the same code from the previous example)

            # Display the result
            plt.imshow(image)
            plt.title(f"Waste Image: {filename}")
            plt.show()

"""Bar plot to visualize the number of training images in each class.

This code provides a clear overview of the distribution of images across different classes in your training dataset
"""

# number of images in each class for training datasets
# Dictionary to store the count of images in each class
data_dic = {}
# Path to the training dataset
curr_path='/content/drive/MyDrive/train'
# Iterate through each folder (class) in the training dataset
for folder in os.listdir(curr_path):
    # Count the number of images in the current class
    data_dic[folder] = len(os.listdir(curr_path + '/' + folder))
# Convert the dictionary to a pandas Series
data_df= pd.Series(data_dic)
# Plot the bar chart
plt.figure(figsize = (15, 6))
data_df.sort_values().plot(kind = 'bar')
plt.xlabel('Training Classes')
plt.ylabel('Number of Traingn images')

"""Define waste classes

This dictionary associates the numeric labels 1, 2, and 3 with the waste classes 'plastic', 'paper', and 'metal', respectively.
"""

classes = { 1: 'plastic',2: 'paper',3: 'metal'}

"""Define waste categories for classification

This dictionary associates the numeric labels 0, 1, 2, and 3 with the waste categories 'Not Waste', 'Plastic', 'Paper', and 'Metal', respectively.
"""

waste_classes = {0: 'Not Waste', 1: 'Plastic', 2: 'Paper', 3: 'Metal'}

"""Test Dataset"""

import os
# Test Dataset
test_data_dir = '/content/drive/MyDrive/test'
# Iterate through the files in the test dataset and run object detection
for filename in os.listdir(test_data_dir):
    if filename.endswith(".jpg") or filename.endswith(".png"):
        input_image_path = os.path.join(test_data_dir, filename)
        detect_objects(input_image_path)

# number of images in each class for testing datasets
data_dic = {}
curr_path='/content/drive/MyDrive/test'
for folder in os.listdir(curr_path):
    data_dic[folder] = len(os.listdir(curr_path + '/' + folder))
# Initialize a pandas Series with the data
data_df= pd.Series(data_dic)
# Plot the bar chart
plt.figure(figsize = (15, 6))
data_df.sort_values().plot(kind = 'bar')
plt.xlabel('Testing Classes')
plt.ylabel('Number of Testing images')

"""Outtput The Results

calculate_precision_recall_f1 for evaluating the precision, recall, and F1 score of a model's predictions.

what is bounding boxes and why there is a need for us in this project?

Bounding boxes are rectangular frames that are used to enclose and identify objects within an image. They define the spatial extent of an object and are commonly employed in computer vision tasks, particularly in object detection.

In my project, bounding boxes allow my model to not only classify the type of waste but also provide information about where in the image the waste is located. This information is valuable for waste management tasks, such as identifying the distribution of different types of waste in a disposal area.

Bounding boxes are a fundamental concept in object detection, providing a standardized way to represent and locate objects in images.

In my project, they enable the MobileNetV2 model to perform accurate waste classification and localization.
"""

def calculate_precision_recall_f1(y_true, y_pred, threshold=0.5):
  """
    Calculate precision, recall, and F1 score for object detection.

    Parameters:
    - y_true: List of true bounding boxes (ground truth).
    - y_pred: List of predicted bounding boxes.
    - threshold: Intersection over Union (IoU) threshold for considering a detection as correct.

    Returns:
    - precision: Precision score.
    - recall: Recall score.
    - f1: F1 score.
    """
    true_positives = 0
    false_positives = 0
    false_negatives = 0
    # Iterate through true and predicted bounding box pairs
    for true_box, pred_box in zip(y_true, y_pred):
      # Compute Intersection over Union (IoU)
        iou = calculate_iou(true_box, pred_box)
        # Check if IoU is above the threshold for a true positive
        if iou >= threshold:
        if iou >= threshold:
            true_positives += 1
        else:
            false_negatives += 1
    # Calculate precision, recall, and F1 score
    # Smoothing Term (1e-7): A small smoothing term (1e-7) is added to avoid division by zero errors.
    precision = true_positives / (true_positives + false_positives + 1e-7)
    recall = true_positives / (true_positives + false_negatives + 1e-7)
    f1 = 2 * (precision * recall) / (precision + recall + 1e-7)

    return precision, recall, f1

"""Output Explanation:

Precision: Precision is a measure of the accuracy of the positive predictions. It is calculated as the ratio of true positives to the sum of true positives and false positives. Precision is high when the model makes fewer false positive predictions.

Recall: Recall, also known as sensitivity or true positive rate, measures the ability of the model to capture all the relevant instances. It is calculated as the ratio of true positives to the sum of true positives and false negatives. Recall is high when the model captures a large proportion of the true positive instances.

F1 Score: The F1 score is the harmonic mean of precision and recall. It provides a balance between precision and recall and is particularly useful when there is an uneven class distribution. F1 score is high when both precision and recall are high.

Interpretation of the output

[0.9999999000000099, 0.0, 0.9999999000000099],
[0.9999999000000099, 0.0, 0.9999999000000099],
[0.9999998500000123, 0.0, 0.9999998500000123]


For the first class: Precision is very high (close to 1), recall is 0 (no true positives), and the F1 score is also very high.

For the second class: Precision is very high (close to 1), recall is 0 (no true positives), and the F1 score is also very high.

For the third class: Precision is very high (close to 1), recall is 0 (no true positives), and the F1 score is also very high.

Need of improvement(Future Scope):
Recall is zero->t may be due to a lack of diversity in the training data.
Solution->We could adjust model parameters, training data, or evaluation criteria based on the observed results.


"""

precision, recall, f1

"""This code is useful for visualizing the training progress of a machine learning model, assessing how well it generalizes to unseen data (validation set), and identifying potential issues such as overfitting or underfitting.

Explanation Of Curves

Training Loss

The training loss is a measure of how well the model is performing on the training dataset during each epoch of training.

The goal during training is to minimize the training loss. A decreasing training loss indicates that the model is learning to make more accurate predictions on the training data.

A very low training loss may suggest that the model is overfitting the training data.


In my curve->We have a decreasing training loss which indicates that the model is learning to make more accurate predictions on the training data.


Validation Loss

The validation loss is a measure of how well the model generalizes to unseen data.

The validation loss helps assess how well the model is expected to perform on new, previously unseen data. It serves as an indicator of the model's ability to generalize beyond the training set.


An increasing validation loss while the training loss is decreasing may indicate overfitting.


So in my curve-> yes there is a minor issue of overfitting as i have increasing validation loss while the training loss is decreasing. But just a minor issue->not a large ,so assumption could be made and this minor issue could be ignored as of now.

Training Accuracy

The training accuracy is a metric that measures the percentage of correctly classified examples in the training dataset. It indicates how well the model is performing on the data it was trained on.

The goal during training is to maximize training accuracy. A high training accuracy suggests that the model is learning to correctly classify examples in the training set.

So in my curve we have a good training accuracy ,so my goal is fulfilled.

Validation Accuracy

The validation accuracy is a metric that measures the percentage of correctly classified examples in a separate validation dataset.

The validation accuracy helps assess how well the model is expected to perform on new, previously unseen data.

If both training and validation accuracy are increasing over epochs, it suggests that the model is learning and generalizing well.


So in my curve we have both training and validation accuracy are increasing over epochs , so my goal is fulfilled.
"""

import matplotlib.pyplot as plt

# Plot training loss
plt.plot(history.history['loss'], label='Training Loss')
# Plot validation loss
if 'val_loss' in history.history:
    plt.plot(history.history['val_loss'], label='Validation Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.title('Training and Validation Loss')
plt.show()

# Plot training accuracy
plt.plot(history.history['accuracy'], label='Training Accuracy')
# Plot validation accuracy
if 'val_accuracy' in history.history:
    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()
plt.title('Training and Validation Accuracy')
plt.show()

"""Explanation Of Curves

Testing Loss

Testing loss measures the error or discrepancy between the model's predictions and the actual target values on the testing dataset. It is a quantitative measure of how well the model generalizes to new, unseen data.

The primary purpose of testing loss is to assess how well the model is expected to perform on real-world data that it has never encountered during training. It helps evaluate the model's ability to generalize beyond the training set.

A lower testing loss indicates better performance.

So in my curve we have a lower testing loss , so the goal is fulfilled.

Testing accuracy
Testing accuracy, also known as test accuracy, is a metric that measures the percentage of correctly classified examples in a separate testing dataset. This metric is used to evaluate how well a trained machine learning model performs on data it has not seen during the training process.

Testing accuracy is calculated as the ratio of the number of correctly classified examples to the total number of examples in the testing dataset. It represents the proportion of instances on which the model makes correct predictions.

The primary purpose of testing accuracy is to assess the model's ability to generalize to new, unseen data. It provides an indication of how well the model is expected to perform in real-world scenarios.

A higher testing accuracy is desirable, indicating that the model is making accurate predictions on the testing dataset.

In my curve we have a higher testing accuracy so the goal is fulfilled.

Validation Loss

Validation loss is a metric used in machine learning to evaluate the performance of a model on a separate validation dataset during the training process. It measures the error or discrepancy between the model's predictions and the actual target values on this independent dataset.


Validation loss is a quantitative measure of how well a model generalizes to data that it has not seen during training.

The primary purpose of validation loss is to provide an unbiased evaluation of the model's performance on data it has not been trained on. It serves as a proxy for the model's ability to generalize to new, unseen examples.

A lower validation loss is indicative of better generalization.It suggests that the model is making accurate predictions on data it has not been exposed to during the training process.


In my curve Validation loss is also decreasing almost fulfilling the desired goal.

Validation Accuracy

Validation accuracy is a metric used in machine learning to assess the performance of a model on a separate validation dataset during the training process. It represents the percentage of correctly classified examples in the validation dataset and serves as an indicator of how well the model generalizes to new, unseen data.

Validation accuracy is calculated as the ratio of the number of correctly classified examples to the total number of examples in the validation dataset. It measures the proportion of instances on which the model makes correct predictions on the validation data.

The primary purpose of validation accuracy is to provide an unbiased evaluation of the model's ability to generalize. It assesses how well the model is expected to perform on new, previously unseen examples, which were not used for training.

A higher validation accuracy is desirable, indicating that the model is making accurate predictions on the validation dataset.

In my curve->Validation accuracy is first increasing then decreasing but then again increasing and so on. So effectively we have a higher validation accuracy , so the goal is fulfilled.
"""

# Plot testing loss
if 'loss' in history.history:
    plt.plot(history.history['loss'], label='Testing Loss')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.legend()
    plt.title('Testing Loss')
    plt.show()

# Plot testing accuracy
if 'accuracy' in history.history:
    plt.plot(history.history['accuracy'], label='Testing Accuracy')
    plt.xlabel('Epochs')
    plt.ylabel('Accuracy')
    plt.legend()
    plt.title('Testing Accuracy')
    plt.show()

# Plot validation loss
if 'val_loss' in history.history:
    plt.plot(history.history['val_loss'], label='Validation Loss')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.legend()
    plt.title('Validation Loss')
    plt.show()

# Plot validation accuracy
if 'val_accuracy' in history.history:
    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
    plt.xlabel('Epochs')
    plt.ylabel('Accuracy')
    plt.legend()
    plt.title('Validation Accuracy')
    plt.show()

"""The model.summary() method provides a concise summary of the architecture and parameters of the model. It displays information about each layer in the model, including the type of layer, output shape, number of parameters, and whether the parameters are trainable.


"""

print(model.summary())

"""Conclusion

Dataset Splitting

Data Augmentation

Model Architecture Using MobileNetV2 as the Base model for Transfer Learning

Custom Model Head

Freezing Layers

Model Compilation

Training

Loading Trained Model

Image Loading and Display

Class Distribution Visualization

Precision, Recall, F1 Score

Plots


"""